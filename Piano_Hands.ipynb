{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
          "kernelId": ""
        },
        "id": "SiTIpPjArIyr"
      },
      "source": [
        "# Piano Hands (ver. 1.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "Credit for GPT2-RGA code used in this colab goes out @ Sashmark97 https://github.com/Sashmark97/midigen and @ Damon Gwinn https://github.com/gwinndr/MusicTransformer-Pytorch\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2022\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GPU CHECK)"
      ],
      "metadata": {
        "id": "sWuJGvJcM1xL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "39411b40-9e39-416e-8fe4-d40f733e7956",
          "kernelId": ""
        },
        "id": "lw-4aqV3sKQG"
      },
      "outputs": [],
      "source": [
        "#@title nvidia-smi gpu check\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
          "kernelId": ""
        },
        "id": "gOd93yV0sGd2"
      },
      "source": [
        "# (SETUP ENVIRONMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
          "kernelId": ""
        },
        "id": "fX12Yquyuihc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install all dependencies (run only once per session)\n",
        "\n",
        "!git clone https://github.com/asigalov61/Piano-Hands\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "\n",
        "!pip install torch-summary\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
          "kernelId": ""
        },
        "id": "z7n9vnKmug1J"
      },
      "outputs": [],
      "source": [
        "#@title Import all needed modules\n",
        "\n",
        "print('Loading needed modules. Please wait...')\n",
        "import os\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import secrets\n",
        "from collections import OrderedDict\n",
        "\n",
        "if not os.path.exists('/content/Dataset'):\n",
        "    os.makedirs('/content/Dataset')\n",
        "\n",
        "print('Loading TMIDIX and GPT2RGAX modules...')\n",
        "os.chdir('/content/Piano-Hands/')\n",
        "import TMIDIX\n",
        "from GPT2RGAX import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from sklearn import metrics\n",
        "\n",
        "os.chdir('/content/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "20b8698a-0b4e-4fdb-ae49-24d063782e77",
          "kernelId": ""
        },
        "id": "ObPxlEutsQBj"
      },
      "source": [
        "# (FROM SCRATCH) Download and process MIDI dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and unzip ASAP Dataset MIDI scores set\n",
        "\n",
        "#@markdown https://github.com/fosfrancesco/asap-dataset\n",
        "\n",
        "#@markdown PLEASE NOTE: The dataset is quite small so results may vary\n",
        "%cd /content/Dataset\n",
        "!wget --no-check-certificate -O 'ASAP-Dataset-MIDI-Scores.zip' \"https://onedrive.live.com/download?cid=8A0D502FC99C608F&resid=8A0D502FC99C608F%2118750&authkey=AHVj3_h3exo_tcY\"\n",
        "!unzip 'ASAP-Dataset-MIDI-Scores.zip'\n",
        "!rm 'ASAP-Dataset-MIDI-Scores.zip'\n",
        "%cd /content/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CdTieodxLxji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (PROCESS)"
      ],
      "metadata": {
        "id": "JwrqQeie08t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process MIDIs with TMIDIX MIDI processor\n",
        "\n",
        "sorted_or_random_file_loading_order = False # Sorted order is NOT usually recommended\n",
        "dataset_ratio = 1 # Change this if you need more data\n",
        "\n",
        "\n",
        "print('TMIDIX MIDI Processor')\n",
        "print('Starting up...')\n",
        "###########\n",
        "\n",
        "files_count = 0\n",
        "\n",
        "gfiles = []\n",
        "\n",
        "events_matrix_final = []\n",
        "\n",
        "times = []\n",
        "\n",
        "###########\n",
        "\n",
        "print('Loading MIDI files...')\n",
        "print('This may take a while on a large dataset in particular.')\n",
        "\n",
        "dataset_addr = \"/content/Dataset\"\n",
        "# os.chdir(dataset_addr)\n",
        "filez = list()\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
        "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
        "print('=' * 70)\n",
        "\n",
        "if filez == []:\n",
        "    print('Could not find any MIDI files. Please check Dataset dir...')\n",
        "    print('=' * 70)\n",
        "\n",
        "print('Randomizing file list...')\n",
        "random.shuffle(filez)\n",
        "\n",
        "print('Processing MIDI files. Please wait...')\n",
        "for f in tqdm(filez[:int(len(filez) * dataset_ratio)]):\n",
        "    try:\n",
        "        fn = os.path.basename(f)\n",
        "        fn1 = fn.split('.')[0]\n",
        "\n",
        "        #print('Loading MIDI file...')\n",
        "        score = TMIDIX.midi2score(open(f, 'rb').read())\n",
        "\n",
        "        \n",
        "        itrack = 1\n",
        "\n",
        "        events_matrix1 = []\n",
        "        events_matrix2 = []\n",
        "\n",
        "        for event in score[1]:         \n",
        "           if event[0] == 'note':\n",
        "              events_matrix1.append(event)\n",
        "\n",
        "        for event in score[2]:         \n",
        "           if event[0] == 'note':\n",
        "              events_matrix2.append(event)\n",
        "        \n",
        "        events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "        events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "        events_matrix2.sort(key=lambda x: x[4], reverse=True)\n",
        "        events_matrix2.sort(key=lambda x: x[1])       \n",
        "\n",
        "\n",
        "        if len(events_matrix1) > 0 and len(events_matrix2) > 0:\n",
        "          avg_ptc1 = sum([y[4] for y in events_matrix1]) / len([y[4] for y in events_matrix1])\n",
        "          avg_ptc2 = sum([y[4] for y in events_matrix2]) / len([y[4] for y in events_matrix2])\n",
        "\n",
        "          if avg_ptc1 > avg_ptc2:\n",
        "            #print('found')\n",
        "\n",
        "            events_matrix3 = []\n",
        "\n",
        "            for e in events_matrix1:\n",
        "              e.extend(['right'])\n",
        "              events_matrix3.append(e)\n",
        "             \n",
        "            for e in events_matrix2:\n",
        "              e.extend(['left'])\n",
        "              events_matrix3.append(e)\n",
        "\n",
        "            events_matrix3.sort(key=lambda x: x[4], reverse=True)\n",
        "            events_matrix3.sort(key=lambda x: x[1])\n",
        "\n",
        "            events_matrix4 = [131]\n",
        "\n",
        "            pe = events_matrix3[0]\n",
        "            for e in events_matrix3[1:]:\n",
        "              time = e[1]-pe[1]\n",
        "\n",
        "              ptc = max(1, min(127, e[4]))\n",
        "\n",
        "              hand = e[6]\n",
        "\n",
        "              if hand == 'right':\n",
        "                handt = 129\n",
        "              else:\n",
        "                handt = 128\n",
        "\n",
        "              #events_matrix4.extend([ptc, handt])\n",
        "              \n",
        "              if time == 0:\n",
        "                events_matrix4.extend([ptc, handt])\n",
        "\n",
        "              else:\n",
        "               events_matrix4.extend([130])\n",
        "               events_matrix4.extend([ptc, handt])\n",
        "\n",
        "              pe = e\n",
        "\n",
        "            events_matrix_final.extend(events_matrix4)\n",
        "\n",
        "            files_count += 1\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print('Saving current progress and quitting...')\n",
        "        break  \n",
        "\n",
        "    except:\n",
        "        print('Bad MIDI:', f)\n",
        "        continue\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')   \n",
        "print('=' * 70)\n",
        "\n",
        "print('Resulting Stats:')\n",
        "print('=' * 70)\n",
        "print('Total good MIDI Files:', files_count)\n",
        "print('=' * 70)\n",
        "\n",
        "print('Total INTs:', len(events_matrix_final))\n",
        "print('Minimum INT:', min(events_matrix_final))\n",
        "print('Maximum INT:', max(events_matrix_final))\n",
        "print('Unique INTs:', len(set(events_matrix_final)))\n",
        "print('Zero INTs:', events_matrix_final.count(0))\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "xkHKfUoucXJk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (LOAD INTs)"
      ],
      "metadata": {
        "id": "eMrq1osy3_ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load processed INTs\n",
        "\n",
        "SEQ_LEN = max_seq\n",
        "\n",
        "BATCH_SIZE = 4 # Change this to your specs\n",
        "\n",
        "# DO NOT FORGET TO ADJUST MODEL PARAMS IN GPT2RGAX module to your specs\n",
        "\n",
        "print('=' * 50)\n",
        "print('Loading training data...')\n",
        "\n",
        "data_train, data_val = torch.Tensor(events_matrix_final), torch.Tensor(events_matrix_final[:8193])\n",
        "\n",
        "class MusicSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        idx = random.randint(0, (self.data.size(0) - self.seq_len-1))\n",
        "\n",
        "        x = self.data[idx: idx + self.seq_len].long()\n",
        "        trg = self.data[(idx+1): (idx+1) + self.seq_len].long()\n",
        "        \n",
        "        return x, trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0)\n",
        "\n",
        "train_dataset = MusicSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = MusicSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "val_loader    = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "print('=' * 50)\n",
        "print('Total INTs in the dataset', len(events_matrix_final))\n",
        "print('Total unique INTs in the dataset', len(set(events_matrix_final)))\n",
        "print('Max INT in the dataset', max(events_matrix_final))\n",
        "print('Min INT in the dataset', min(events_matrix_final))\n",
        "print('=' * 50)\n",
        "print('Length of the dataset:',len(train_dataset))\n",
        "print('Number of batched samples per epoch:', len(events_matrix_final) // max_seq // BATCH_SIZE)\n",
        "print('=' * 50)\n",
        "print('Sample train dataset:', train_dataset[0])\n",
        "print('Sample val dataset:', val_dataset[0])\n",
        "print('=' * 50)\n",
        "print('Train loader length:', len(train_loader))\n",
        "print('Val loader length:', len(val_loader))\n",
        "print('=' * 50)\n",
        "print('Done! Enjoy! :)')\n",
        "print('=' * 50)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XFf-J37qnsI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkVqviDzJOrv"
      },
      "source": [
        "# (TRAIN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9CBW8xYupH8"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "4aa21407-a3e9-4ed2-9bf1-83c295482b8a",
          "kernelId": ""
        },
        "id": "2moo7uUmpxvC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train\n",
        "\n",
        "DIC_SIZE = 132\n",
        "\n",
        "# DO NOT FORGET TO ADJUST MODEL PARAMS IN GPT2RGAX module to your specs\n",
        "\n",
        "config = GPTConfig(DIC_SIZE, \n",
        "                   max_seq,\n",
        "                   dim_feedforward=512,\n",
        "                   n_layer=32, \n",
        "                   n_head=32, \n",
        "                   n_embd=512,\n",
        "                   enable_rpr=True,\n",
        "                   er_len=max_seq)\n",
        "\n",
        "# DO NOT FORGET TO ADJUST MODEL PARAMS IN GPT2RGAX module to your specs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPT(config)\n",
        "\n",
        "model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#=====\n",
        "\n",
        "init_step = 0\n",
        "lr = LR_DEFAULT_START\n",
        "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
        "eval_loss_func = nn.CrossEntropyLoss(ignore_index=DIC_SIZE)\n",
        "train_loss_func = eval_loss_func\n",
        "\n",
        "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
        "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
        "\n",
        "\n",
        "#===\n",
        "\n",
        "best_eval_acc        = 0.0\n",
        "best_eval_acc_epoch  = -1\n",
        "best_eval_loss       = float(\"inf\")\n",
        "best_eval_loss_epoch = -1\n",
        "best_acc_file = '/content/gpt2_rpr_acc.pth'\n",
        "best_loss_file = '/content/gpt2_rpr_loss.pth'\n",
        "loss_train, loss_val, acc_val = [], [], []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    new_best = False\n",
        "    \n",
        "    loss = train(epoch+1, \n",
        "                 model, train_loader, \n",
        "                 train_loss_func, \n",
        "                 opt, \n",
        "                 lr_scheduler, \n",
        "                 num_iters=-1, \n",
        "                 save_checkpoint_steps=1000)\n",
        "    \n",
        "    loss_train.append(loss)\n",
        "    \n",
        "    eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)\n",
        "    loss_val.append(eval_loss)\n",
        "    acc_val.append(eval_acc)\n",
        "    \n",
        "    if(eval_acc > best_eval_acc):\n",
        "        best_eval_acc = eval_acc\n",
        "        best_eval_acc_epoch  = epoch+1\n",
        "        torch.save(model.state_dict(), best_acc_file)\n",
        "        new_best = True\n",
        "\n",
        "    if(eval_loss < best_eval_loss):\n",
        "        best_eval_loss       = eval_loss\n",
        "        best_eval_loss_epoch = epoch+1\n",
        "        torch.save(model.state_dict(), best_loss_file)\n",
        "        new_best = True\n",
        "    \n",
        "    if(new_best):\n",
        "        print(\"Best eval acc epoch:\", best_eval_acc_epoch)\n",
        "        print(\"Best eval acc:\", best_eval_acc)\n",
        "        print(\"\")\n",
        "        print(\"Best eval loss epoch:\", best_eval_loss_epoch)\n",
        "        print(\"Best eval loss:\", best_eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "72338f3f-34c4-40e3-a48a-42ed9729466a",
          "kernelId": ""
        },
        "id": "R4LIXk1vHX92",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Eval funct to eval separately if needed\n",
        "\n",
        "\n",
        "#=====\n",
        "\n",
        "init_step = 0\n",
        "lr = LR_DEFAULT_START\n",
        "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
        "eval_loss_func = nn.CrossEntropyLoss(ignore_index=DIC_SIZE)\n",
        "train_loss_func = eval_loss_func\n",
        "\n",
        "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
        "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
        "\n",
        "\n",
        "eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKFoeke9L7H"
      },
      "source": [
        "# (SAVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "id": "73bea62d-084b-4f9a-9e55-2b34a932a7a4",
          "kernelId": ""
        },
        "id": "gqyDatHC9X1z"
      },
      "outputs": [],
      "source": [
        "#@title Save the model\n",
        "\n",
        "print('Saving the model...')\n",
        "full_path_to_model_checkpoint = \"/content/Piano-Hands-Trained-Model.pth\" #@param {type:\"string\"}\n",
        "torch.save(model.state_dict(), full_path_to_model_checkpoint)\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "-35ALDrjdP4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (UNZIP PRE-TRAINED MODEL)"
      ],
      "metadata": {
        "id": "qW2vQv99dR5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip pre-trained Piano Hands Model\n",
        "%cd /content/Piano-Hands/Model\n",
        "\n",
        "print('=' * 70)\n",
        "print('Unzipping pre-trained model...Please wait...')\n",
        "print('=' * 70)\n",
        "\n",
        "!cat /content/Piano-Hands/Model/Piano_Hands_Trained_Model.zip* > Piano_Hands_Trained_Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "!unzip -j Piano_Hands_Trained_Model.zip\n",
        "print('=' * 70)\n",
        "\n",
        "print('Done! Enjoy! :)')\n",
        "print('=' * 70)\n",
        "%cd /content/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DN8En4sFdZOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (LOAD/RELOAD)"
      ],
      "metadata": {
        "id": "7sR7qdKqLgUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LOAD/RELOAD Piano Hands  Model\n",
        "full_path_to_trained_model_checkpoint = \"/content/Piano-Hands/Model/Piano_Hands_Trained_Model_13940_steps_0.3038_loss.pth\" #@param {type:\"string\"}\n",
        "\n",
        "print('Loading Piano Hands model...')\n",
        "config = GPTConfig(132, \n",
        "                  512,\n",
        "                  dim_feedforward=512,\n",
        "                  n_layer=32, \n",
        "                  n_head=32, \n",
        "                  n_embd=512,\n",
        "                  enable_rpr=True,\n",
        "                  er_len=512)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPT(config)\n",
        "\n",
        "state_dict = torch.load(full_path_to_trained_model_checkpoint, map_location=device)\n",
        "\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[7:] #remove 'module'\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "summary(model)\n",
        "\n",
        "cos_sim = metrics.pairwise.cosine_similarity(\n",
        "    model.tok_emb.weight.detach().cpu().numpy()\n",
        ")\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"none\")\n",
        "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
        "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
        "plt.xlabel(\"Position\")\n",
        "plt.ylabel(\"Position\")\n",
        "plt.tight_layout()\n",
        "plt.plot()\n",
        "plt.savefig(\"/content/Piano-Hands-Tokens-Embeddings-Plot.png\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "sJNMy4yQwX3T",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GENERATE)"
      ],
      "metadata": {
        "id": "LICeDSpnynoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Piano MIDI\n",
        "full_path_to_MIDI_file = \"/content/Piano-Hands/Piano-Hands-Sample-MIDI-1.mid\" #@param {type:\"string\"}\n",
        "\n",
        "print('Loading MIDI file...')\n",
        "\n",
        "score = TMIDIX.midi2ms_score(open(full_path_to_MIDI_file, 'rb').read())\n",
        "\n",
        "events_matrix = []\n",
        "\n",
        "itrack = 1\n",
        "\n",
        "while itrack < len(score):\n",
        "    for event in score[itrack]:         \n",
        "        if event[0] == 'note' and event[3] != 9:\n",
        "            events_matrix.append(event)\n",
        "    itrack += 1\n",
        "\n",
        "events_matrix.sort(key=lambda x: x[4], reverse=True)\n",
        "events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "cho = []\n",
        "chords = []\n",
        "notes = []\n",
        "\n",
        "pe = events_matrix[0]\n",
        "for e in events_matrix:\n",
        "  time = e[1] - pe[1]\n",
        "  dur = e[2]\n",
        "  ptc = e[4]\n",
        "  vel = e[5]\n",
        "\n",
        "  notes.append([time, dur, ptc, vel])\n",
        "\n",
        "  if time == 0:\n",
        "    cho.append([time, dur, ptc, vel])\n",
        "  else:\n",
        "    chords.append(cho)\n",
        "    cho = []\n",
        "    cho.append([time, dur, ptc, vel])\n",
        "\n",
        "  pe = e\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G4FbZQIwyosQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Piano Hands Parts\n",
        "\n",
        "#@markdown Increasing number of batches may help to improve precision\n",
        "\n",
        "number_of_batches = 1 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "model_temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "\n",
        "print('=' * 70)\n",
        "print('Piano Hands Model Generator')\n",
        "print('=' * 70)\n",
        "\n",
        "print('Generating...')\n",
        "\n",
        "inp = [131]\n",
        "\n",
        "out1 = []\n",
        "\n",
        "for i in tqdm(range(len(chords))):\n",
        "\n",
        "  inp.append(130)\n",
        "\n",
        "  for j in range(len(chords[i])):\n",
        "\n",
        "    inp.append(chords[i][j][2])\n",
        "\n",
        "    out = model.generate_batches(torch.Tensor(inp[-511:]), \n",
        "                      target_seq_length=len(inp[-511:])+1,\n",
        "                      num_batches=number_of_batches, \n",
        "                      temperature=model_temperature, verbose=False).tolist()\n",
        "\n",
        "    ou = [y[-1] for y in out]\n",
        "\n",
        "    o = max(set(ou), key = ou.count)\n",
        "\n",
        "    out1.extend([o])\n",
        "\n",
        "print('Done!')\n",
        "print('=' * 70)\n",
        "\n",
        "if len(notes) != 0:\n",
        "    \n",
        "    song = notes\n",
        "    song_f = []\n",
        "    time = 0\n",
        "    dur = 0\n",
        "    vel = 0\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "\n",
        "    son = []\n",
        "\n",
        "    song1 = []\n",
        "    count = 0\n",
        "    for s in song[:len(out1)]:\n",
        "\n",
        "        if out1[count] == 128:\n",
        "          channel = 1 # Left Hand Channel\n",
        "        else:\n",
        "          channel = 0 # Right Hand Channel\n",
        "\n",
        "        time += s[0]\n",
        "            \n",
        "        dur = s[1]\n",
        "        \n",
        "        pitch = s[2]\n",
        "\n",
        "        vel = s[3]\n",
        "                                  \n",
        "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                        output_signature = 'Piano Hands',  \n",
        "                                                        output_file_name = '/content/Piano-Hands-Music-Composition', \n",
        "                                                        track_name='Project Los Angeles',\n",
        "                                                        list_of_MIDI_patches=[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                                        number_of_ticks_per_quarter=500)\n",
        "\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JxrVBqM_0Thr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzCMd94Tu_gz"
      },
      "source": [
        "# Congrats! You did it! :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}